# Awesome-Parameter-Efficient-Fine-Tuning-for-Foundation-Models

## PEFT paper lists

### [Content](#content)

<table>

<tr><td><a href="#Selective">1. Selective</a></td></tr>
<tr><td><a href="#Additive">2. Additive</a></td></tr> 
<tr><td><a href="#Prompt">3. Prompt</a></td></tr>
<tr><td><a href="#Reparameters">4. Reparameters</a></td></tr>
<tr><td><a href="#Hybrid">5. Hybrid</a></td></tr>

</table>
<!-- ** **. . '18. [paper]() -->



### [Selective](#content)

#### LLM

**Revealing the Dark Secrets of BERT**. **EMNLP-IJCNLP'2019**.

[paper](https://www.aminer.cn/pub/5d63adc33a55ac410be32803/revealing-the-dark-secrets-of-bert) 

*Olga Kovaleva,Alexey Romanov,Anna Rogers,Anna Rumshisky*

#### VFM

#### VFM

#### VLM

#### VGM

#### MFM

### [Addictive](#content)

#### LLM

##### Adapter

1. **Parameter-Efficient Transfer Learning for NLP**. **CoRR'2019**. 

  *Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly*.

  [paper](https://www.aminer.cn/pub/5c61606ae1cd8eae1501e0f5/parameter-efficient-transfer-learning-for-nlp)

2. **Exploring versatile generative language model via parameter-efficient transfer learning**. **EMNLP'2020**. 

  *Lin Zhaojiang, Madotto Andrea, Fung Pascale*. 

  [paper](https://www.aminer.cn/pub/5e8ef2ae91e011679da0f112/exploring-versatile-generative-language-model-via-parameter-efficient-transfer-learning)

3. **MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer**. **Intelligent Systems In Accounting,
 Finance &Management'2020**. 

  *Pfeiffer Jonas, Vulić Ivan,Gurevych Iryna, Ruder Sebastian*. 
 
  [paper](https://www.aminer.cn/pub/5eafe7e091e01198d3986542/mad-x-an-adapter-based-framework-for-multi-task-cross-lingual-transfer)

4. **Counter-Interference Adapter for Multilingual Machine Translation**. **EMNLP'2021**.

  *Yaoming Zhu, Jiangtao Feng, Chengqi Zhao, Mingxuan Wang, Lei Li*.
  
  [paper](https://aminer.cn/pub/619799ec91e011c8223730c6/counter-interference-adapter-for-multilingual-machine-translation)

5. **AdapterDrop - On the Efficiency of Adapters in Transformers**. **EMNLP'2021**.

  *Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, Iryna Gurevych*.
  
  [paper](https://www.aminer.cn/pub/5f92b9db91e011edb3573b95/adapterdrop-on-the-efficiency-of-adapters-in-transformers)

6. **Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters**. **EMNLP'2022**.

  *Hongyu Zhao, Hao Tan, Hongyuan Mei*.
  
  [paper](https://www.aminer.cn/pub/636482d890e50fcafdccb0cc/Tiny-Attention%20Adapter:%20Contexts%20Are%20More%20Important%20Than%20the%20Number%20of%20Parameters)


7. **Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks**.*Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, James Henderson*.**EMNLP'2022**.[paper](https://www.aminer.cn/pub/60c2da8091e0117e30ca2817/parameter-efficient-multi-task-fine-tuning-for-transformers-via-shared-hypernetworks)


8. **BAD-X: Bilingual Adapters Improve Zero-Shot Cross-Lingual Transfer**._Marinela Parovic, Goran Glavas, Ivan Vulic, Anna Korhonen_.**NAACL'2022**.[paper](https://www.aminer.cn/pub/634d80f190e50fcafd4ef432/bad-x-bilingual-adapters-improve-zero-shot-cross-lingual-transfer)


9. **AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning**. _Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, Jianfeng Gao_.**EMNLP'2022**.[paper](https://www.aminer.cn/pub/628ef0485aee126c0f82d92e/AdaMix:%20Mixture-of-Adaptations%20for%20Parameter-efficient%20Model%20Tuning)


10. **AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks**. _Chin-Lun Fu, Zih-Ching Chen, Yun-Ru Lee, Hung-yi Lee_.**NAACL'2022**.[paper](https://www.aminer.cn/pub/62708f615aee126c0fa69008/adapterbias-parameter-efficient-token-dependent-representation-shift-for-adapters-in-nlp-tasks)


11. **SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters**. _Shwai He, Liang Ding, Daize Dong, Miao Zhang, Dacheng Tao_.**arXiv'2022**.[paper](https://www.aminer.cn/pub/6344dede90e50fcafd24d1cc/sparseadapter-an-easy-approach-for-improving-the-parameter-efficiency-of-adapters)


12. **LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning**. _Yi-Lin Sung, Jaemin Cho, Mohit Bansal_. **NeurIPS'2022**.[paper](https://www.aminer.cn/pub/62a94e065aee126c0f9c02cd/lst-ladder-side-tuning-for-parameter-and-memory-efficient-transfer-learning)


13. **MerA: Merging Pretrained Adapters For Few-Shot Learning**. _Shwai He, Run-Ze Fan, Liang Ding, Li Shen, Tianyi Zhou, Dacheng Tao_. **CoRR'2023**. [paper](https://www.aminer.cn/pub/64f00ff43fda6d7f06ecec7d/mera-merging-pretrained-adapters-for-few-shot-learning)


14. **AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models**. _Alexandra Chronopoulou, Matthew E. Peters, Alexander Fraser, Jesse Dodge_. **arXiv'2023**. [paper](https://www.aminer.cn/pub/63ec4dcd90e50fcafd66b158/adaptersoup-weight-averaging-to-improve-generalization-of-pretrained-language-models)


15. **Hadamard Adapter: An Extreme Parameter-Efficient Adapter Tuning Method for Pre-trained Language Models**. _Yuyan Chen, Qiang Fu, Ge Fan, Lun Du, Jian-Guang Lou, Shi Han, Dongmei Zhang, Zhixu Li, Yanghua_. **CIKM'2023**. [paper](https://www.aminer.cn/pub/65360d69939a5f4082b0a4b3/hadamard-adapter-an-extreme-parameter-efficient-adapter-tuning-method-for-pre-trained)


16. **Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference**. _Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Y. Zhao, Yuexin Wu, Bo Li, Yu Zhang, Ming-Wei Chang_. **NeurIPS'2023**. [paper](https://www.aminer.cn/pub/643621a190e50fcafd665e94/conditional-adapters-parameter-efficient-transfer-learning-with-fast-inference)


#### VFM

#### VFM

#### VLM

#### VGM

#### MFM


### [Prompt](#content)

#### LLM

1. **Prefix-Tuning: Optimizing Continuous Prompts for Generation**. **ACL'2021**.

  _Xiang Lisa Li, Percy Liang_.
 
  [paper](https://www.aminer.cn/pub/5ff4336291e01130648dc2f4/prefix-tuning-optimizing-continuous-prompts-for-generation)

2. **The Power of Scale for Parameter-Efficient Prompt Tuning**. **EMNLP'2021**.

  _Brian Lester, Rami Al-Rfou’, Noah Constant_.  
  
  [paper](https://www.aminer.cn/pub/607ffd8d91e011772654f712/the-power-of-scale-for-parameter-efficient-prompt-tuning)

#### VFM

#### VFM

#### VLM

#### VGM

#### MFM


### [Reparameters](#content)

#### LLM

#### VFM

#### VFM

#### VLM

#### VGM

#### MFM


### [Hybrid](#content)

#### LLM

#### VFM

#### VFM

#### VLM

#### VGM

#### MFM


## Citation

If you find our survey and repository helpful, please kindly cite our paper:
