# Awesome-Parameter-Efficient-Fine-Tuning-for-Foundation-Models

## PEFT paper lists

### [Content](#content)

<table>

<tr><td><a href="#Selective">1. Selective</a></td></tr>
<tr><td><a href="#Additive">2. Additive</a></td></tr> 
<tr><td><a href="#Prompt">3. Prompt</a></td></tr>
<tr><td><a href="#Reparameters">4. Reparameters</a></td></tr>
<tr><td><a href="#Hybrid">5. Hybrid</a></td></tr>

</table>
<!-- ** **. . '18. [paper]() -->



### [Selective](#content)

#### LLM

##### Specific Selection

1.**Revealing the Dark Secrets of BERT**. **EMNLP-IJCNLP'2019**.

  [paper](https://www.aminer.cn/pub/5d63adc33a55ac410be32803/revealing-the-dark-secrets-of-bert) 

  *Olga Kovaleva,Alexey Romanov,Anna Rogers,Anna Rumshisky*

2.**BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models**. **ACL'2022**.
 
[paper](https://www.aminer.cn/pub/60d1586491e011c16f0cb484/bitfit-simple-parameter-efficient-fine-tuning-for-transformer-based-masked-language-models)

 *Elad Ben-Zaken, Shauli Ravfogel, Yoav Goldberg*

3.**Parameter-Efficient Tuning with Special Token Adaptation**. **EACL'2023**.

[paper](https://www.aminer.cn/pub/6392ac4190e50fcafd9f650e/parameter-efficient-tuning-with-special-token-adaptation)

*Xiaocong Yang, James Y. Huang, Wenxuan Zhou, Muhao Chen*

##### Automatic Selection

1.**Masking As an Efficient Alternative to Finetuning for Pretrained Language Models**. **EMNLP'2020.**

[paper](https://www.aminer.cn/pub/5ea8009091e0111d387ee767/masking-as-an-efficient-alternative-to-finetuning-for-pretrained-language-models)

*Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, Hinrich Schutze*

2.**AutoFreeze: Automatically Freezing Model Blocks to Accelerate Fine-tuning**. **arXiv'2021**.

[paper](https://www.aminer.cn/pub/601a774291e0111e8877d8fd/autofreeze-automatically-freezing-model-blocks-to-accelerate-fine-tuning)

*Yuhan Liu, Saurabh Agarwal, Shivaram Venkataraman*

3.**Parameter-Efficient Transfer Learning with Diff Pruning**. **计算机科学'2021**.

[paper](https://www.aminer.cn/pub/5fd8af0f91e0119b22c1f3a8/parameter-efficient-transfer-learning-with-diff-pruning)

*Demi Guo, Alexander M. Rush, Yoon Kim*

4.**Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning**. **EMNLP'2021**.

[paper](https://aminer.cn/pub/614012c15244ab9dcb8166d0/raise-a-child-in-large-language-model-towards-effective-and-generalizable-fine)

*Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, Fei Huang*

5.**Training Neural Networks with Fixed Sparse Masks**. **NIPS'2021**.

[paper](https://www.aminer.cn/pub/619715fd5244ab9dcb185a4d/training-neural-networks-with-fixed-sparse-masks)

*Yi-Lin Sung, Varun Nair, Colin Raffel*

#### VFM


#### MFM

### [Addictive](#content)

#### LLM

##### Adapter

1. **Parameter-Efficient Transfer Learning for NLP**. **CoRR'2019**. 

  *Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly*.

  [paper](https://www.aminer.cn/pub/5c61606ae1cd8eae1501e0f5/parameter-efficient-transfer-learning-for-nlp)

2. **Exploring versatile generative language model via parameter-efficient transfer learning**. **EMNLP'2020**. 

  *Lin Zhaojiang, Madotto Andrea, Fung Pascale*. 

  [paper](https://www.aminer.cn/pub/5e8ef2ae91e011679da0f112/exploring-versatile-generative-language-model-via-parameter-efficient-transfer-learning)

3. **MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer**. **Intelligent Systems In Accounting,
 Finance &Management'2020**. 

  *Pfeiffer Jonas, Vulić Ivan,Gurevych Iryna, Ruder Sebastian*. 
 
  [paper](https://www.aminer.cn/pub/5eafe7e091e01198d3986542/mad-x-an-adapter-based-framework-for-multi-task-cross-lingual-transfer)

4. **Counter-Interference Adapter for Multilingual Machine Translation**. **EMNLP'2021**.

  *Yaoming Zhu, Jiangtao Feng, Chengqi Zhao, Mingxuan Wang, Lei Li*.
  
  [paper](https://aminer.cn/pub/619799ec91e011c8223730c6/counter-interference-adapter-for-multilingual-machine-translation)

5. **AdapterDrop - On the Efficiency of Adapters in Transformers**. **EMNLP'2021**.

  *Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, Iryna Gurevych*.
  
  [paper](https://www.aminer.cn/pub/5f92b9db91e011edb3573b95/adapterdrop-on-the-efficiency-of-adapters-in-transformers)

6. **Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters**. **EMNLP'2022**.

  *Hongyu Zhao, Hao Tan, Hongyuan Mei*.
  
  [paper](https://www.aminer.cn/pub/636482d890e50fcafdccb0cc/Tiny-Attention%20Adapter:%20Contexts%20Are%20More%20Important%20Than%20the%20Number%20of%20Parameters)


7. **Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks**.*Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, James Henderson*.**EMNLP'2022**.[paper](https://www.aminer.cn/pub/60c2da8091e0117e30ca2817/parameter-efficient-multi-task-fine-tuning-for-transformers-via-shared-hypernetworks)


8. **BAD-X: Bilingual Adapters Improve Zero-Shot Cross-Lingual Transfer**._Marinela Parovic, Goran Glavas, Ivan Vulic, Anna Korhonen_.**NAACL'2022**.[paper](https://www.aminer.cn/pub/634d80f190e50fcafd4ef432/bad-x-bilingual-adapters-improve-zero-shot-cross-lingual-transfer)


9. **AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning**. _Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, Jianfeng Gao_.**EMNLP'2022**.[paper](https://www.aminer.cn/pub/628ef0485aee126c0f82d92e/AdaMix:%20Mixture-of-Adaptations%20for%20Parameter-efficient%20Model%20Tuning)


10. **AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks**. _Chin-Lun Fu, Zih-Ching Chen, Yun-Ru Lee, Hung-yi Lee_.**NAACL'2022**.[paper](https://www.aminer.cn/pub/62708f615aee126c0fa69008/adapterbias-parameter-efficient-token-dependent-representation-shift-for-adapters-in-nlp-tasks)


11. **SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters**. _Shwai He, Liang Ding, Daize Dong, Miao Zhang, Dacheng Tao_.**arXiv'2022**.[paper](https://www.aminer.cn/pub/6344dede90e50fcafd24d1cc/sparseadapter-an-easy-approach-for-improving-the-parameter-efficiency-of-adapters)


12. **LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning**. _Yi-Lin Sung, Jaemin Cho, Mohit Bansal_. **NeurIPS'2022**.[paper](https://www.aminer.cn/pub/62a94e065aee126c0f9c02cd/lst-ladder-side-tuning-for-parameter-and-memory-efficient-transfer-learning)


13. **MerA: Merging Pretrained Adapters For Few-Shot Learning**. _Shwai He, Run-Ze Fan, Liang Ding, Li Shen, Tianyi Zhou, Dacheng Tao_. **CoRR'2023**. [paper](https://www.aminer.cn/pub/64f00ff43fda6d7f06ecec7d/mera-merging-pretrained-adapters-for-few-shot-learning)


14. **AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models**. _Alexandra Chronopoulou, Matthew E. Peters, Alexander Fraser, Jesse Dodge_. **arXiv'2023**. [paper](https://www.aminer.cn/pub/63ec4dcd90e50fcafd66b158/adaptersoup-weight-averaging-to-improve-generalization-of-pretrained-language-models)


15. **Hadamard Adapter: An Extreme Parameter-Efficient Adapter Tuning Method for Pre-trained Language Models**. _Yuyan Chen, Qiang Fu, Ge Fan, Lun Du, Jian-Guang Lou, Shi Han, Dongmei Zhang, Zhixu Li, Yanghua_. **CIKM'2023**. [paper](https://www.aminer.cn/pub/65360d69939a5f4082b0a4b3/hadamard-adapter-an-extreme-parameter-efficient-adapter-tuning-method-for-pre-trained)


16. **Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference**. _Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Y. Zhao, Yuexin Wu, Bo Li, Yu Zhang, Ming-Wei Chang_. **NeurIPS'2023**. [paper](https://www.aminer.cn/pub/643621a190e50fcafd665e94/conditional-adapters-parameter-efficient-transfer-learning-with-fast-inference)


#### VFM

#### VFM

#### VLM

#### VGM

#### MFM


### [Prompt](#content)

#### LLM

1. **Prefix-Tuning: Optimizing Continuous Prompts for Generation**. **ACL'2021**.

  _Xiang Lisa Li, Percy Liang_.
 
  [paper](https://www.aminer.cn/pub/5ff4336291e01130648dc2f4/prefix-tuning-optimizing-continuous-prompts-for-generation)

2. **The Power of Scale for Parameter-Efficient Prompt Tuning**. **EMNLP'2021**.

  _Brian Lester, Rami Al-Rfou’, Noah Constant_.  
  
  [paper](https://www.aminer.cn/pub/607ffd8d91e011772654f712/the-power-of-scale-for-parameter-efficient-prompt-tuning)

#### VFM

#### VFM

#### VLM

#### VGM

#### MFM


### [Reparameters](#content)

#### LLM

#### VFM

#### VFM

#### VLM

#### VGM

#### MFM


### [Hybrid](#content)

#### LLM

#### VFM

#### VFM

#### VLM

#### VGM

#### MFM


## Citation

If you find our survey and repository helpful, please kindly cite our paper:
